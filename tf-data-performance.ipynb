{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5436669-0a49-46cc-984f-0be799f18572",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde410df-3789-411a-b850-0e1ac225c1eb",
   "metadata": {},
   "source": [
    "Start with defining a class inheriting from tf.data.Dataset called ArtificialDataset. This dataset:\n",
    "\n",
    "- Generates num_samples samples (default is 3)\n",
    "- Sleeps for some time before the first item to simulate opening a file\n",
    "- Sleeps for some time before producing each item to simulate reading data from a file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53208a47-63ce-4521-a107-ab667385d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialDataset(tf.data.Dataset):\n",
    "    def _generator(num_samples):\n",
    "        # Opening the file\n",
    "        time.sleep(0.03)\n",
    "\n",
    "        for sample_idx in range(num_samples):\n",
    "            # Reading data (line, record) from the file\n",
    "            time.sleep(0.015)\n",
    "\n",
    "            yield (sample_idx,)\n",
    "\n",
    "    def __new__(cls, num_samples=3):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_signature = tf.TensorSpec(shape = (1,), dtype = tf.int64),\n",
    "            args=(num_samples,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5207199-49e7-4b7f-8928-54186a59408a",
   "metadata": {},
   "source": [
    "Next, write a dummy training loop that measures how long it takes to iterate over a dataset. Training time is simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c644d5-4834-4515-9bbc-95065e17760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440127e-81d2-4302-8f77-bde9362a4e45",
   "metadata": {},
   "source": [
    "Start with a naive pipeline using no tricks, iterating over the dataset as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2f356e-dd1f-4ed7-b3ab-d0bbf9fa35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.27612568624317646\n"
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a5cc7-6343-4ccb-be69-3d3d1e901968",
   "metadata": {},
   "source": [
    "However, in a naive synchronous implementation like here, while your pipeline is fetching the data, your model is sitting idle. Conversely, while your model is training, the input pipeline is sitting idle. The training step time is thus the sum of opening, reading and training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000616-6e78-497e-88e2-3deee06da7a8",
   "metadata": {},
   "source": [
    "__Prefetching__\n",
    "\n",
    "Prefetching overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\n",
    "\n",
    "The tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to tf.data.AUTOTUNE, which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
    "\n",
    "Note that the prefetch transformation provides benefits any time there is an opportunity to overlap the work of a \"producer\" with the work of a \"consumer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb19e74-a546-4982-8ffa-288a8730c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.23265091236680746\n"
     ]
    }
   ],
   "source": [
    "# Try prefetch\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ede17-7df2-49d2-972c-512f0a7a4e7b",
   "metadata": {},
   "source": [
    "__Interleave__<br>\n",
    "To mitigate the impact of the various data extraction overheads, the tf.data.Dataset.interleave transformation can be used to parallelize the data loading step, interleaving the contents of other datasets (such as data file readers). The number of datasets to overlap can be specified by the cycle_length argument, while the level of parallelism can be specified by the num_parallel_calls argument. Similar to the prefetch transformation, the interleave transformation supports tf.data.AUTOTUNE, which will delegate the decision about what level of parallelism to use to the tf.data runtime.\n",
    "\n",
    "__Sequential interleave__<br>\n",
    "The default arguments of the tf.data.Dataset.interleave transformation make it interleave single samples from two datasets sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63d80bf-75d2-4b22-a51b-bd6277501032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.5298015046864748\n"
     ]
    }
   ],
   "source": [
    "# Interleave: Sequential\n",
    "\n",
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(lambda _: ArtificialDataset())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c019c09-61e2-46cd-8cb3-a9f2f4b223b4",
   "metadata": {},
   "source": [
    "__Parallel interleave__<br>\n",
    "Now, use the num_parallel_calls argument of the interleave transformation. This loads multiple datasets in parallel, reducing the time waiting for the files to be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8d51e7-5088-4f8c-9793-5ea8b342e58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.3163652131333947\n"
     ]
    }
   ],
   "source": [
    "# Interleave: Parallelizing data extraction\n",
    "\n",
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDataset(),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd898b0f-d8ba-4a3b-8e16-9f8a7800efe9",
   "metadata": {},
   "source": [
    "__Parallelizing data transformation__<br>\n",
    "When preparing data, input elements may need to be pre-processed. To this end, the tf.data API offers the tf.data.Dataset.map transformation, which applies a user-defined function to each element of the input dataset. Because input elements are independent of one another, the pre-processing can be parallelized across multiple CPU cores. To make this possible, similarly to the prefetch and interleave transformations, the map transformation provides the num_parallel_calls argument to specify the level of parallelism.\n",
    "\n",
    "Choosing the best value for the num_parallel_calls argument depends on your hardware, characteristics of your training data (such as its size and shape), the cost of your map function, and what other processing is happening on the CPU at the same time. A simple heuristic is to use the number of available CPU cores. However, as for the prefetch and interleave transformation, the map transformation supports tf.data.AUTOTUNE which will delegate the decision about what level of parallelism to use to the tf.data runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f541244-7a6c-4a5f-9bcf-41af989f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_function(s):\n",
    "    # Do some hard pre-processing\n",
    "    tf.py_function(lambda: time.sleep(0.03), [], ())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe6120-fc09-4653-8f83-ddfab7413bda",
   "metadata": {},
   "source": [
    "__Sequential mapping__<br>\n",
    "Start by using the map transformation without parallelism as a baseline example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb77c5d-58ff-4798-9ebd-580afed8283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.45774794556200504\n"
     ]
    }
   ],
   "source": [
    "# Sequential data transformation\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .map(mapped_function)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb17ea-8149-451f-bd1d-96fe74c92a89",
   "metadata": {},
   "source": [
    "__Parallel mapping__<br>\n",
    "Now, use the same pre-processing function but apply it in parallel on multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549ce654-5b4b-4084-8dc1-23d0e0c818f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.29934894014149904\n"
     ]
    }
   ],
   "source": [
    "# parallel data transformation\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .map(\n",
    "        mapped_function,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad91b7-487c-4c6b-b433-d7079b5ee352",
   "metadata": {},
   "source": [
    "__Caching__<br>\n",
    "The tf.data.Dataset.cache transformation can cache a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed5d4f4-40de-4fab-b156-462b1374e1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.37716342508792877\n"
     ]
    }
   ],
   "source": [
    "# Caching\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .map(  # Apply time consuming operations before cache\n",
    "        mapped_function\n",
    "    ).cache(\n",
    "    ),\n",
    "    5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33047b08-764f-4630-8614-0581b2d0c49f",
   "metadata": {},
   "source": [
    "If the user-defined function passed into the map transformation is expensive, apply the cache transformation after the map transformation as long as the resulting dataset can still fit into memory or local storage. If the user-defined function increases the space required to store the dataset beyond the cache capacity, either apply it after the cache transformation or consider pre-processing your data before your training job to reduce resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb5bdf-2373-4c3d-9161-fd4f9430fe81",
   "metadata": {},
   "source": [
    "__Vectorizing mapping__<br>\n",
    "Invoking a user-defined function passed into the map transformation has overhead related to scheduling and executing the user-defined function. Vectorize the user-defined function (that is, have it operate over a batch of inputs at once) and apply the batch transformation before the map transformation.\n",
    "\n",
    "To illustrate this good practice, your artificial dataset is not suitable. The scheduling delay is around 10 microseconds (10e-6 seconds), far less than the tens of milliseconds used in the ArtificialDataset, and thus its impact is hard to see.\n",
    "\n",
    "For this example, use the base tf.data.Dataset.range function and simplify the training loop to its simplest form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ae39ce-e060-406b-8b92-707ead046287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_dataset = tf.data.Dataset.range(10000)\n",
    "\n",
    "def fast_benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in tf.data.Dataset.range(num_epochs):\n",
    "        for _ in dataset:\n",
    "            pass\n",
    "    tf.print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "\n",
    "def increment(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4813c37f-dfc7-46fc-b123-783502e1786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.25147498585283756\n"
     ]
    }
   ],
   "source": [
    "# Scalar mapping\n",
    "fast_benchmark(\n",
    "    fast_dataset\n",
    "    # Apply function one item at a time\n",
    "    .map(increment)\n",
    "    # Batch\n",
    "    .batch(256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dcc9e85-0a96-4fa1-8b75-371c7bb576bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.03382870554924011\n"
     ]
    }
   ],
   "source": [
    "# Vectorized mapping\n",
    "fast_benchmark(\n",
    "    fast_dataset\n",
    "    .batch(256)\n",
    "    # Apply function on a batch of items\n",
    "    # The tf.Tensor.__add__ method already handle batches\n",
    "    .map(increment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4fd4d-7346-4484-9de8-7cc6c840f6a7",
   "metadata": {},
   "source": [
    "__Reducing memory footprint__<br>\n",
    "A number of transformations, including interleave, prefetch, and shuffle, maintain an internal buffer of elements. If the user-defined function passed into the map transformation changes the size of the elements, then the ordering of the map transformation and the transformations that buffer elements affects the memory usage. In general, choose the order that results in lower memory footprint, unless different ordering is desirable for performance.\n",
    "\n",
    "__Caching partial computations__<br>\n",
    "It is recommended to cache the dataset after the map transformation except if this transformation makes the data too big to fit in memory. A trade-off can be achieved if your mapped function can be split in two parts: a time consuming one and a memory consuming part. In this case, you can chain your transformations like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e85637-9082-408b-a4e6-cbf69cdfb9d8",
   "metadata": {},
   "source": [
    "__Best practice summary__<br>\n",
    "Here is a summary of the best practices for designing performant TensorFlow input pipelines:\n",
    "\n",
    "- Use the prefetch transformation to overlap the work of a producer and consumer\n",
    "- Parallelize the data reading transformation using the interleave transformation\n",
    "- Parallelize the map transformation by setting the num_parallel_calls argument\n",
    "- Use the cache transformation to cache data in memory during the first epoch\n",
    "- Vectorize user-defined functions passed in to the map transformation\n",
    "- Reduce memory usage when applying the interleave, prefetch, and shuffle transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427a2f6-70b5-444d-a68c-3a77ba7e6033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
